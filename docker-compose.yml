version: '3.1'

networks:
  project_network:
    external: true
services:
  db:
    container_name: source_database
    restart: always
    shm_size: 2gb
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
    image: postgres:14.1
    env_file:
      - .env.postgres
    environment:
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
        - ./pg_data:/var/lib/postgresql/data/pgdata
        - ./backups:/var/backups
    networks:
      - project_network
    ports:
      - "5432:5432"
    command:
      - "postgres"
      - "-c"
      - "work_mem=100MB"
      - "-c"
      - "max_connections=40"
      - "-c"
      - "shared_buffers=1000MB"
      - "-c"
      - "maintenance_work_mem=512MB"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "min_wal_size=4GB"
      - "-c"
      - "max_wal_size=16GB"
      - "-c"
      - "effective_cache_size=4GB"
      - "-c"
      - "wal_level=logical"

  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.3
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - project_network

  kafka:
    image: confluentinc/cp-enterprise-kafka:5.5.3
    container_name: kafka
    depends_on: [ zookeeper ]
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9991
    ports:
      - "29092:29092"
      - "9092:9092"
    networks:
      - project_network

  debezium:
    container_name: debezium
    image: debezium/connect:1.4
    env_file:
      - .env.debezium
    depends_on: [ kafka, schema-registry ]
    ports:
      - 8083:8083
    networks:
      - project_network

  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.3
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081,http://localhost:8081
    ports:
      - 8081:8081
    depends_on: [ zookeeper, kafka ]
    networks:
      - project_network

  control-center:
    image: confluentinc/cp-enterprise-control-center:7.4.0
    hostname: control-center
    container_name: control-center
    depends_on:
      - kafka
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'kafka:9092'
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      CONFLUENT_METRICS_ENABLE: 'false'
      PORT: 9021
    networks:
      - project_network


  spark-master:
    container_name: ingestao-spark-master
    build: .
    image: bitnami/spark:3.4.0
    entrypoint: [ './entrypoint.sh', 'master' ]
    networks:
      - project_network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ./book_data:/opt/spark/data
      - ./spark_apps:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events
    env_file:
      - .env.spark
    ports:
      - '9090:8080'
      - '7077:7077'


  spark-history-server:
    container_name: ingestao-spark-history
    image: bitnami/spark:3.4.0
    entrypoint: [ './entrypoint.sh', 'history' ]
    networks:
      - project_network
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - spark-logs:/opt/spark/spark-events
    ports:
      - '18080:18080'

  spark-worker:
    #    container_name: da-spark-worker
    image: bitnami/spark:3.4.0
    entrypoint: [ './entrypoint.sh', 'worker' ]
    networks:
      - project_network
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - ./book_data:/opt/spark/data
      - ./spark_apps:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events

volumes:
  spark-logs: