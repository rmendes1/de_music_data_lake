{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5024cb21-7c12-44b5-809b-8994e2744e60",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports e Funções"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import lit\n",
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "from delta import DeltaTable\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"../lib/\")     \n",
    "sys.path.append(f'/Workspace/Users/{dbutils.widgets.get(\"account\")}/music_data_lake/src/lib')                           \n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2d12b6-da67-4e3c-a54b-f74142047020",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"bronze\"\n",
    "schema = \"music_data\"\n",
    "tablename = dbutils.widgets.get(\"tablename\")\n",
    "primary_key = dbutils.widgets.get(\"primary_key\")\n",
    "timestamp_field = \"ts_ms\"\n",
    "\n",
    "if not utils.table_exists(spark, catalog=catalog, schema=schema, table=tablename):\n",
    "    print(\"Tabela não existente, criando...\")\n",
    "\n",
    "    df_full = spark.read.format(\"parquet\").load(f'/Volumes/raw/{schema}/full_load/{tablename}/')\n",
    "\n",
    "    (df_full.coalesce(1)\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(f'{catalog}.{schema}.{tablename}')\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(f\"Tabela {tablename} já existente, ignorando full-load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f85a2ee-3628-45ed-9c6a-6716f80dae99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Caminhos dos arquivos Parquet\n",
    "table_schema = utils.import_schema(tablename)\n",
    "bronze = DeltaTable.forName(spark, f\"{catalog}.{schema}.{tablename}\")\n",
    "\n",
    "\n",
    "def upsert(df, deltatable):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "    # df_stream.createOrReplaceGlobalTempView(f\"view_{tablename}\")  \n",
    "\n",
    "    # query = f\"\"\"\n",
    "    #     SELECT *\n",
    "    #     FROM global_temp.view_{tablename}\n",
    "    #     WHERE {primary_key} IS NOT NULL\n",
    "    #     QUALIFY ROW_NUMBER() OVER (PARTITION BY {primary_key} ORDER BY {timestamp_field} DESC) = 1\n",
    "\n",
    "    # \"\"\"\n",
    "\n",
    "    # df_cdc = spark.sql(query)\n",
    "    \n",
    "    ########################## Realizar o MERGE combinando upsert e delete ##################################\n",
    "    # Filtra os registros onde a chave primária não é nula\n",
    "    df_filtered = df.filter(f\"{primary_key} IS NOT NULL\")\n",
    "\n",
    "    # Ordena os registros por chave primária e timestamp, e mantém apenas o mais recente\n",
    "    windowSpec = Window.partitionBy(primary_key).orderBy(col(timestamp_field).desc())\n",
    "    df_cdc = df_filtered.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    "                        .filter(col(\"row_number\") == 1) \\\n",
    "                        .drop(\"row_number\")\n",
    "\n",
    "\n",
    "    (deltatable.alias(\"target\")\n",
    "               .merge(df_cdc.alias(\"source\"), f\"target.{primary_key} = source.{primary_key}\")\n",
    "               .whenMatchedDelete(condition = \"source.operation = 'd'\")\n",
    "               .whenMatchedUpdateAll(condition = \"source.operation = 'u'\")\n",
    "               .whenNotMatchedInsertAll(condition = \"source.operation = 'c' OR source.operation = 'u'\")\n",
    "               .execute()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c271ea-edab-4a06-98b0-79d0615fe858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_stream = (spark.readStream\n",
    "              .format(\"parquet\")\n",
    "              .option(\"cloudFiles.format\", \"parquet\")\n",
    "              # .option(\"cloudFiles.maxFilesPerTrigger\", \"500\")\n",
    "              .schema(table_schema)\n",
    "              .load(f\"/Volumes/raw/{schema}/cdc/postgres.public.{tablename}/\"))\n",
    "\n",
    "# Converte release_date de dias desde a época Unix para date\n",
    "df_stream = df_stream.withColumn(\"release_date\", expr(\"DATE_FROM_UNIX_DATE(release_date)\"))\n",
    "\n",
    "# Converte created_at e updated_at de microssegundos para timestamp\n",
    "df_stream = df_stream.withColumn(\"created_at\", expr(\"from_unixtime(created_at / 1e6)\")) \\\n",
    "                     .withColumn(\"updated_at\", expr(\"from_unixtime(updated_at / 1e6)\"))\n",
    "\n",
    "# Converte ts_ms de milissegundos para timestamp\n",
    "df_stream = df_stream.withColumn(\"ts_ms\", expr(\"from_unixtime(ts_ms / 1000)\"))\n",
    "\n",
    "stream = (df_stream.writeStream\n",
    "                   .option(\"checkpointLocation\", f\"/Volumes/raw/{schema}/cdc/postgres.public.{tablename}/{tablename}_checkpoint/\")\n",
    "                   .foreachBatch(lambda df, batchID: upsert(df, bronze))\n",
    "                   .trigger(availableNow=True)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb18293-5ac3-4b47-bb3c-dd71b48baac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = stream.start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8137535469985219,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
